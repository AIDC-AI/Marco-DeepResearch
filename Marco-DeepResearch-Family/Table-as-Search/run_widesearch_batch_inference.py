#!/usr/bin/env python3
# Copyright (C) 2026 AIDC-AI
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
æ‰¹é‡æ¨ç†è¿è¡Œè„šæœ¬ - æ”¯æŒå¹¶è¡Œè¿è¡Œå¤šä¸ª widesearch ä»»åŠ¡
å¤ç”¨ run_widesearch_inference.py çš„ run_inference å‡½æ•°

æ³¨æ„ï¼šä½¿ç”¨ multiprocessing è€Œé threading æ¥å®ç°çœŸæ­£çš„è¶…æ—¶æ§åˆ¶
å› ä¸º Python çš„çº¿ç¨‹æ— æ³•è¢«å¼ºåˆ¶ç»ˆæ­¢ï¼Œåªæœ‰è¿›ç¨‹å¯ä»¥è¢« terminate/kill
"""

# ============================================================================
# âš ï¸ CRITICAL: Load environment variables FIRST, before any other imports!
# ============================================================================
from tools.env_loader import load_dotenv
load_dotenv(override=True)

import argparse
import json
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict
import traceback
import multiprocessing
from multiprocessing import Process, Queue

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
BASE_DIR = Path(__file__).parent.absolute()
sys.path.insert(0, str(BASE_DIR))

# å¯¼å…¥ run_inference å‡½æ•°å’Œ clear_database_tables å‡½æ•°
from run_widesearch_inference import run_inference, clear_database_tables
from rich.console import Console
from tools.dataloader import load_widesearch_dataset
from pymongo import MongoClient

console = Console()


def get_tables_for_task(db_name: str, instance_id: str, connection_string: str = None) -> Dict:
    """
    ä»æ•°æ®åº“è·å–æŒ‡å®šä»»åŠ¡çš„æ‰€æœ‰è¡¨æ ¼æ•°æ®
    
    Args:
        db_name: æ•°æ®åº“åç§°
        instance_id: ä»»åŠ¡å®ä¾‹ IDï¼ˆç”¨ä½œè¡¨åå‰ç¼€ï¼‰
        connection_string: MongoDB è¿æ¥å­—ç¬¦ä¸²
        
    Returns:
        Dict: {table_name: {"columns": [...], "records": [...]}, ...}
    """
    if connection_string is None:
        connection_string = os.getenv("MONGODB_CONNECTION_STRING", "mongodb://localhost:27017/")
    
    tables = {}
    
    try:
        client = MongoClient(connection_string)
        db = client[db_name]
        
        # è·å–æ‰€æœ‰ä»¥ instance_id ä¸ºå‰ç¼€çš„é›†åˆ
        collection_names = db.list_collection_names()
        prefix = f"{instance_id}_"
        
        for collection_name in collection_names:
            # è·³è¿‡ç³»ç»Ÿé›†åˆå’Œå…ƒæ•°æ®é›†åˆ
            if collection_name.startswith("system.") or collection_name == "_metadata":
                continue
            
            # åªè·å–è¯¥ä»»åŠ¡çš„è¡¨ï¼ˆä»¥ instance_id_ å¼€å¤´ï¼‰
            if not collection_name.startswith(prefix):
                continue
            
            collection = db[collection_name]
            
            # è·å– schema æ–‡æ¡£
            schema_doc = collection.find_one({"_id": "__schema__"})
            if schema_doc is None:
                continue
            
            columns = schema_doc.get("columns", [])
            
            # è·å–æ‰€æœ‰è®°å½•ï¼ˆæ’é™¤ schema æ–‡æ¡£ï¼‰
            records = list(collection.find({"_id": {"$ne": "__schema__"}}))
            
            # æå–è¡¨åï¼ˆå»æ‰å‰ç¼€ï¼‰
            table_name_without_prefix = collection_name[len(prefix):]
            
            tables[table_name_without_prefix] = {
                "full_name": collection_name,
                "columns": columns,
                "records": records,
                "record_count": len(records)
            }
        
        client.close()
        
    except Exception as e:
        console.print(f"[bold red]Error getting tables for task {instance_id}: {e}[/bold red]")
        traceback.print_exc()
    
    return tables


def tables_to_markdown(tables: Dict) -> str:
    """
    å°†è¡¨æ ¼æ•°æ®è½¬æ¢ä¸º Markdown æ ¼å¼
    
    Args:
        tables: {table_name: {"columns": [...], "records": [...]}, ...}
        
    Returns:
        str: Markdown æ ¼å¼çš„è¡¨æ ¼å­—ç¬¦ä¸²
    """
    if not tables:
        return "No tables found for this task."
    
    markdown_parts = []
    
    for table_name, table_data in tables.items():
        columns = table_data.get("columns", [])
        records = table_data.get("records", [])
        record_count = table_data.get("record_count", len(records))
        
        # æ·»åŠ è¡¨æ ¼æ ‡é¢˜
        #markdown_parts.append(f"## Table: {table_name}")
        #markdown_parts.append(f"*Total records: {record_count}*\n")
        
        #if not columns:
        #    markdown_parts.append("*No columns defined*\n")
        #    continue
        
        #if not records:
        #    markdown_parts.append(f"*Columns: {', '.join(columns)}*")
        #    markdown_parts.append("*No records in this table*\n")
        #    continue
        
        # åˆ›å»º Markdown è¡¨æ ¼å¤´
        header = "| " + " | ".join(columns) + " |"
        separator = "| " + " | ".join(["---"] * len(columns)) + " |"
        
        markdown_parts.append(header)
        markdown_parts.append(separator)
        
        # æ·»åŠ æ•°æ®è¡Œ
        for record in records:
            row_values = []
            for col in columns:
                value = record.get(col, "")
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶å¤„ç†ç‰¹æ®Šå­—ç¬¦
                str_value = str(value) if value is not None else ""
                # æ›¿æ¢å¯èƒ½ç ´åè¡¨æ ¼æ ¼å¼çš„å­—ç¬¦
                str_value = str_value.replace("|", "\\|").replace("\n", " ").replace("\r", "")
                # é™åˆ¶å•å…ƒæ ¼é•¿åº¦ï¼Œé¿å…å¤ªé•¿çš„å†…å®¹
                if len(str_value) > 200:
                    str_value = str_value[:197] + "..."
                row_values.append(str_value)
            
            row = "| " + " | ".join(row_values) + " |"
            markdown_parts.append(row)
        
        markdown_parts.append("")  # è¡¨æ ¼ä¹‹é—´æ·»åŠ ç©ºè¡Œ
    
    return "\n".join(markdown_parts)


def has_api_error_in_log(output_dir: Path, instance_id: str) -> bool:
    """
    æ£€æµ‹ agent_log.txt ä¸­æ˜¯å¦å­˜åœ¨ API é”™è¯¯æ¨¡å¼
    
    è¿™äº›é”™è¯¯é€šå¸¸è¡¨ç¤ºä»»åŠ¡å› ä¸º LLM API æŠ¥é”™è€Œå¤±è´¥ï¼Œä¸åº”è¯¥è¿›è¡Œåå¤„ç†æ¢å¤
    
    Args:
        output_dir: è¾“å‡ºç›®å½•
        instance_id: ä»»åŠ¡å®ä¾‹ ID
        
    Returns:
        bool: True è¡¨ç¤ºå­˜åœ¨ API é”™è¯¯ï¼ŒFalse è¡¨ç¤ºæ²¡æœ‰
    """
    # agent_log.txt çš„è·¯å¾„
    log_file = output_dir / "work" / instance_id / "agent_log.txt"
    
    if not log_file.exists():
        return False
    
    # API é”™è¯¯æ¨¡å¼åˆ—è¡¨
    api_error_patterns = [
        "Error while generating output:",
        "Connection aborted.",
        "ConnectionResetError",
        "Connection reset by peer",
        "APIConnectionError",
        "APITimeoutError",
        "RateLimitError",
        "ServiceUnavailableError",
        "InternalServerError",
        "BadRequestError",
        "AuthenticationError",
        "RemoteDisconnected",
        "ConnectionRefusedError",
        "TimeoutError",
        "SSLError",
    ]
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            # åªè¯»å–æœ€å 5000 ä¸ªå­—ç¬¦æ¥æ£€æµ‹é”™è¯¯ï¼ˆé€šå¸¸é”™è¯¯åœ¨æ—¥å¿—æœ«å°¾ï¼‰
            f.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
            file_size = f.tell()
            read_size = min(5000, file_size)
            f.seek(max(0, file_size - read_size))
            content = f.read()
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½• API é”™è¯¯æ¨¡å¼
        for pattern in api_error_patterns:
            if pattern in content:
                return True
        
        return False
    except Exception as e:
        console.print(f"[yellow]âš ï¸  Failed to read agent log for {instance_id}: {e}[/yellow]")
        return False


def is_result_valid(result: Dict) -> bool:
    """
    æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆï¼ˆä¸éœ€è¦åå¤„ç†æ›¿æ¢ï¼‰
    
    Args:
        result: ä»»åŠ¡ç»“æœå­—å…¸
        
    Returns:
        bool: True è¡¨ç¤ºç»“æœæœ‰æ•ˆï¼ŒFalse è¡¨ç¤ºéœ€è¦åå¤„ç†æ›¿æ¢
    """
    # æ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
    if result.get("error"):
        return False
    
    # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
    if result.get("timeout"):
        return False
    
    # è·å–ç­”æ¡ˆ
    answer = result.get("answer") or result.get("response")
    
    if not answer:
        return False
    
    answer_str = str(answer)
    
    # æ£€æŸ¥æ˜¯å¦ä»¥ Error: å¼€å¤´
    if answer_str.startswith("Error:"):
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·æ ‡ç­¾ï¼ˆè¯´æ˜ä»»åŠ¡è¢«ä¸­æ–­ï¼‰
    if contains_tool_tags(answer_str):
        return False
    
    return True


def postprocess_result(
    result: Dict,
    db_name: str,
    output_dir: Path,
    connection_string: str = None
) -> Dict:
    """
    åå¤„ç†ä»»åŠ¡ç»“æœï¼šå¦‚æœç»“æœæ— æ•ˆï¼Œä»æ•°æ®åº“è·å–è¡¨æ ¼æ•°æ®å¹¶æ›¿æ¢
    
    æ³¨æ„ï¼šå¦‚æœæ£€æµ‹åˆ° API é”™è¯¯ï¼ˆå¦‚è¿æ¥è¢«é‡ç½®ï¼‰ï¼Œåˆ™ä¸è¿›è¡Œæ¢å¤ï¼Œç›´æ¥ç•™ç©º response
    
    Args:
        result: ä»»åŠ¡ç»“æœå­—å…¸
        db_name: æ•°æ®åº“åç§°
        output_dir: è¾“å‡ºç›®å½•
        connection_string: MongoDB è¿æ¥å­—ç¬¦ä¸²
        
    Returns:
        Dict: å¤„ç†åçš„ç»“æœ
    """
    instance_id = result.get("instance_id")
    
    if not instance_id:
        return result
    
    # å¦‚æœç»“æœæœ‰æ•ˆï¼Œä¸éœ€è¦åå¤„ç†
    if is_result_valid(result):
        return result
    
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ API é”™è¯¯ï¼ˆå¦‚è¿æ¥è¢«é‡ç½®ç­‰ï¼‰
    # è¿™ç§æƒ…å†µä¸‹ä¸åº”è¯¥è¿›è¡Œæ¢å¤ï¼Œå› ä¸ºä»»åŠ¡å¯èƒ½è¿˜æ²¡æœ‰çœŸæ­£å¼€å§‹æ‰§è¡Œ
    if has_api_error_in_log(output_dir, instance_id):
        console.print(f"[red]âŒ Task {instance_id} failed due to API error, skipping recovery (response will be empty)[/red]")
        result["answer"] = ""
        result["response"] = ""
        result["api_error"] = True
        result["skip_recovery"] = True
        
        # ä¿å­˜æ›´æ–°åçš„ç»“æœåˆ°æ–‡ä»¶
        result_file = output_dir / f"{instance_id}.json"
        try:
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            console.print(f"[yellow]âš ï¸  Empty result saved to: {result_file}[/yellow]")
        except Exception as e:
            console.print(f"[yellow]âš ï¸  Failed to save result: {e}[/yellow]")
        
        return result
    
    console.print(f"[yellow]âš ï¸  Task {instance_id} has invalid result, attempting to recover from database tables...[/yellow]")
    
    # ä»æ•°æ®åº“è·å–è¯¥ä»»åŠ¡çš„è¡¨æ ¼æ•°æ®
    tables = get_tables_for_task(db_name, instance_id, connection_string)
    
    if not tables:
        console.print(f"[yellow]âš ï¸  No tables found for task {instance_id}, keeping original result[/yellow]")
        return result
    
    # è½¬æ¢ä¸º Markdown æ ¼å¼
    markdown_result = tables_to_markdown(tables)
    
    # æ·»åŠ è¯´æ˜å‰ç¼€
    final_result = f"```markdown\n{markdown_result}\n```"
    
    # æ›´æ–°ç»“æœ
    result["answer"] = final_result
    result["response"] = final_result
    result["recovered_from_tables"] = True
    result["recovered_tables_count"] = len(tables)
    result["original_error"] = result.get("error")
    result["error"] = None  # æ¸…é™¤é”™è¯¯æ ‡è®°ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æ¢å¤äº†ç»“æœ
    
    console.print(f"[green]âœ… Task {instance_id}: Recovered result from {len(tables)} table(s)[/green]")
    
    # ä¿å­˜æ›´æ–°åçš„ç»“æœåˆ°æ–‡ä»¶
    result_file = output_dir / f"{instance_id}.json"
    try:
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        console.print(f"[green]âœ… Updated result saved to: {result_file}[/green]")
    except Exception as e:
        console.print(f"[yellow]âš ï¸  Failed to save updated result: {e}[/yellow]")
    
    return result


def _run_single_task_in_process(
    example: Dict,
    main_model_id: str,
    tabular_model_id: str,
    deep_model_id: str,
    output_dir: str,
    db_name: str,
    clear_db: bool,
    use_summary_tool: bool,
    tool_response_retention_budget: Optional[int],
    max_tool_threads: Optional[int],
    use_out_key: bool,
    global_visit_limit: Optional[int],
    global_search_limit: Optional[int],
    main_max_steps: int,
    subagent_max_steps: int,
    main_enable_context_summarization: bool,
    main_context_token_threshold: int,
    tabular_enable_context_summarization: bool,
    tabular_context_token_threshold: int,
    deep_enable_context_summarization: bool,
    deep_context_token_threshold: int,
    tabular_agent_limit: Optional[int],
    deep_agent_limit: Optional[int],
    result_queue: Queue
):
    """
    åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œå•ä¸ªä»»åŠ¡çš„å‡½æ•°
    
    è¿™ä¸ªå‡½æ•°ä¼šåœ¨ç‹¬ç«‹çš„è¿›ç¨‹ä¸­æ‰§è¡Œï¼Œå¯ä»¥è¢«ä¸»è¿›ç¨‹å¼ºåˆ¶ç»ˆæ­¢
    ç»“æœé€šè¿‡ Queue ä¼ é€’å›ä¸»è¿›ç¨‹
    """
    # åœ¨å­è¿›ç¨‹ä¸­é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
    load_dotenv(override=True)
    
    instance_id = example["instance_id"]
    task_id = example.get("task_id", instance_id)
    query = example["query"]
    
    start_time = datetime.now()
    output = None
    error = None
    run_inference_result = None
    
    process_id = os.getpid()
    print(f"[{instance_id}] ğŸš€ Starting task in process {process_id} at {start_time.strftime('%H:%M:%S')}")
    
    try:
        output_path = Path(output_dir)
        
        # è°ƒç”¨ run_inference
        output = run_inference(
            question=query,
            main_model_id=main_model_id,
            tabular_model_id=tabular_model_id,
            deep_model_id=deep_model_id,
            output_dir=str(output_path),
            db_name=db_name,
            clear_db=clear_db,
            use_summary_tool=use_summary_tool,
            instance_id=instance_id,
            tool_response_retention_budget=tool_response_retention_budget,
            max_tool_threads=max_tool_threads,
            use_out_key=use_out_key,
            global_visit_limit=global_visit_limit,
            global_search_limit=global_search_limit,
            main_max_steps=main_max_steps,
            subagent_max_steps=subagent_max_steps,
            main_enable_context_summarization=main_enable_context_summarization,
            main_context_token_threshold=main_context_token_threshold,
            tabular_enable_context_summarization=tabular_enable_context_summarization,
            tabular_context_token_threshold=tabular_context_token_threshold,
            deep_enable_context_summarization=deep_enable_context_summarization,
            deep_context_token_threshold=deep_context_token_threshold,
            tabular_agent_limit=tabular_agent_limit,
            deep_agent_limit=deep_agent_limit
        )
        
        # è¯»å– run_inference ä¿å­˜çš„ç»“æœæ–‡ä»¶
        result_file = output_path / f"{instance_id}.json"
        if result_file.exists():
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    run_inference_result = json.load(f)
            except:
                run_inference_result = None
                
    except Exception as e:
        error = str(e)
        print(f"[{instance_id}] âŒ ERROR: {type(e).__name__}: {error}")
        traceback.print_exc()
    
    # æ„å»ºç»“æœ
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    if run_inference_result:
        result = run_inference_result.copy()
        result["query"] = query
        result["error"] = error
        result["timeout"] = False
    else:
        result = {
            "instance_id": instance_id,
            "task_id": task_id,
            "query": query,
            "question": query,
            "answer": str(output) if output else None,
            "response": str(output) if output else None,
            "error": error,
            "timeout": False,
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration_seconds": duration,
            "main_model_id": main_model_id,
            "tabular_model_id": tabular_model_id,
            "deep_model_id": deep_model_id,
        }
    
    # å°†ç»“æœæ”¾å…¥é˜Ÿåˆ—
    result_queue.put(result)


def contains_tool_tags(text: str) -> bool:
    """æ£€æŸ¥æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å·¥å…·æ ‡ç­¾ï¼Œè¯´æ˜ä»»åŠ¡è¢«ä¸­æ–­æœªæ­£å¸¸å®Œæˆ"""
    if not text:
        return False
    
    text_str = str(text)
    
    # æ£€æµ‹å¸¸è§çš„å·¥å…·è°ƒç”¨æ ‡ç­¾æ¨¡å¼
    tool_patterns = [
        r'```json',       # ```json (ä»£ç å—å¼€å§‹)
        #r'```python',     # ```python (ä»£ç å—å¼€å§‹)
        r'```\s*\{',      # ``` { ... } (é€šç”¨ä»£ç å—)
        r'Action:\s*',    # Action: ...
        r'Thought:\s*',   # Thought: ...
        r'"name":\s*"[^"]*"',  # JSON ä¸­çš„å·¥å…·åç§°
        r"'name':\s*'[^']*'",  # JSON ä¸­çš„å·¥å…·åç§°
        r"'arguments':\s*\{",  # JSON ä¸­çš„å·¥å…·å‚æ•°
        r'"arguments":\s*\{',  # JSON ä¸­çš„å·¥å…·å‚æ•°
        r'Calling tools' # tool-calling keywords
    ]
    
    for pattern in tool_patterns:
        if re.search(pattern, text_str, re.IGNORECASE | re.MULTILINE):
            return True
    
    return False


def is_task_completed(instance_id: str, output_dir: Path) -> bool:
    """æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²ç»å®Œæˆ - é€šè¿‡æ£€æŸ¥ {instance_id}.json æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    result_file = output_dir / f"{instance_id}.json"
    if result_file.exists():
        # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯é”™è¯¯ï¼‰
        try:
            with open(result_file, 'r', encoding='utf-8') as f:
                result = json.load(f)
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„ç­”æ¡ˆï¼ˆä¸æ˜¯é”™è¯¯ï¼‰
                answer = result.get("answer") or result.get("response")
                if answer:
                    # æ£€æŸ¥æ˜¯å¦ä»¥ Error: å¼€å¤´
                    if str(answer).startswith("Error:"):
                        return False
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å·¥å…·æ ‡ç­¾ï¼ˆè¯´æ˜ä»»åŠ¡è¢«ä¸­æ–­ï¼‰
                    if contains_tool_tags(answer):
                        console.print(f"[yellow]âš ï¸  Task {instance_id} contains tool tags, marking as incomplete[/yellow]")
                        return False
                    return True
        except:
            # å¦‚æœæ–‡ä»¶æŸåï¼Œè®¤ä¸ºæœªå®Œæˆ
            return False
    return False


def run_single_task_with_timeout(
    example: Dict,
    main_model_id: str,
    tabular_model_id: str,
    deep_model_id: str,
    output_dir: Path,
    db_name: str,
    clear_db: bool,
    use_summary_tool: bool,
    tool_response_retention_budget: Optional[int],
    max_tool_threads: Optional[int] = None,
    use_out_key: bool = False,
    global_visit_limit: Optional[int] = None,
    global_search_limit: Optional[int] = None,
    main_max_steps: int = 40,
    subagent_max_steps: int = 40,
    main_enable_context_summarization: bool = False,
    main_context_token_threshold: int = 80000,
    tabular_enable_context_summarization: bool = False,
    tabular_context_token_threshold: int = 60000,
    deep_enable_context_summarization: bool = False,
    deep_context_token_threshold: int = 60000,
    tabular_agent_limit: Optional[int] = None,
    deep_agent_limit: Optional[int] = None,
    timeout_seconds: int = 3600
) -> Dict:
    """è¿è¡Œå•ä¸ªä»»åŠ¡ï¼Œå¸¦çœŸæ­£çš„è¶…æ—¶æ§åˆ¶ï¼ˆä½¿ç”¨ multiprocessingï¼‰
    
    å…³é”®ï¼šä½¿ç”¨ multiprocessing.Process è€Œé threading
    å› ä¸º Python çº¿ç¨‹æ— æ³•è¢«å¼ºåˆ¶ç»ˆæ­¢ï¼Œä½†è¿›ç¨‹å¯ä»¥é€šè¿‡ terminate() æˆ– kill() ç»ˆæ­¢
    """
    instance_id = example["instance_id"]
    task_id = example.get("task_id", instance_id)
    query = example["query"]
    
    start_time = datetime.now()
    
    console.print(f"[cyan][{instance_id}] ğŸš€ Starting task in subprocess at {start_time.strftime('%H:%M:%S')} (timeout: {timeout_seconds}s)[/cyan]")
    
    # åˆ›å»ºç”¨äºè¿›ç¨‹é—´é€šä¿¡çš„é˜Ÿåˆ—
    result_queue = multiprocessing.Queue()
    
    # åˆ›å»ºå­è¿›ç¨‹
    process = Process(
        target=_run_single_task_in_process,
        args=(
            example,
            main_model_id,
            tabular_model_id,
            deep_model_id,
            str(output_dir),
            db_name,
            clear_db,
            use_summary_tool,
            tool_response_retention_budget,
            max_tool_threads,
            use_out_key,
            global_visit_limit,
            global_search_limit,
            main_max_steps,
            subagent_max_steps,
            main_enable_context_summarization,
            main_context_token_threshold,
            tabular_enable_context_summarization,
            tabular_context_token_threshold,
            deep_enable_context_summarization,
            deep_context_token_threshold,
            tabular_agent_limit,
            deep_agent_limit,
            result_queue
        )
    )
    
    # å¯åŠ¨å­è¿›ç¨‹
    process.start()
    
    # ç­‰å¾…å­è¿›ç¨‹å®Œæˆï¼Œå¸¦è¶…æ—¶
    process.join(timeout=timeout_seconds)
    
    # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦ä»åœ¨è¿è¡Œï¼ˆå³è¶…æ—¶äº†ï¼‰
    if process.is_alive():
        console.print(f"[bold red][{instance_id}] â° TIMEOUT after {timeout_seconds}s - Terminating process...[/bold red]")
        
        # å…ˆå°è¯•ä¼˜é›…ç»ˆæ­¢
        process.terminate()
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©è¿›ç¨‹æ¸…ç†
        process.join(timeout=10)
        
        # å¦‚æœè¿˜åœ¨è¿è¡Œï¼Œå¼ºåˆ¶æ€æ­»
        if process.is_alive():
            console.print(f"[bold red][{instance_id}] Process did not terminate, forcing kill...[/bold red]")
            process.kill()
            process.join(timeout=5)
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        # è¿”å›è¶…æ—¶ç»“æœ
        return {
            "instance_id": instance_id,
            "task_id": task_id,
            "query": query,
            "question": query,
            "answer": None,
            "response": None,
            "error": f"Task timeout after {timeout_seconds} seconds (actual duration: {duration:.1f}s)",
            "timeout": True,
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration_seconds": duration,
            "main_model_id": main_model_id,
            "tabular_model_id": tabular_model_id,
            "deep_model_id": deep_model_id,
        }
    
    # è¿›ç¨‹æ­£å¸¸å®Œæˆï¼Œä»é˜Ÿåˆ—è·å–ç»“æœ
    try:
        # ä½¿ç”¨éé˜»å¡æ–¹å¼è·å–ç»“æœï¼Œä»¥é˜²é˜Ÿåˆ—ä¸ºç©º
        if not result_queue.empty():
            result = result_queue.get(timeout=5)
            console.print(f"[bold green][{instance_id}] âœ… Task completed successfully[/bold green]")
            return result
        else:
            # é˜Ÿåˆ—ä¸ºç©ºï¼Œå¯èƒ½æ˜¯è¿›ç¨‹å¼‚å¸¸é€€å‡º
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            console.print(f"[bold yellow][{instance_id}] âš ï¸ Process completed but no result in queue[/bold yellow]")
            
            # å°è¯•è¯»å–ç»“æœæ–‡ä»¶
            result_file = output_dir / f"{instance_id}.json"
            if result_file.exists():
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        result = json.load(f)
                    result["query"] = query
                    result["timeout"] = False
                    return result
                except:
                    pass
            
            return {
                "instance_id": instance_id,
                "task_id": task_id,
                "query": query,
                "question": query,
                "answer": None,
                "response": None,
                "error": "Process completed but returned no result",
                "timeout": False,
                "start_time": start_time.isoformat(),
                "end_time": end_time.isoformat(),
                "duration_seconds": duration,
                "main_model_id": main_model_id,
                "tabular_model_id": tabular_model_id,
                "deep_model_id": deep_model_id,
            }
    except Exception as e:
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        console.print(f"[bold red][{instance_id}] âŒ Error getting result from queue: {e}[/bold red]")
        return {
            "instance_id": instance_id,
            "task_id": task_id,
            "query": query,
            "question": query,
            "answer": None,
            "response": None,
            "error": f"Error getting result: {str(e)}",
            "timeout": False,
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration_seconds": duration,
            "main_model_id": main_model_id,
            "tabular_model_id": tabular_model_id,
            "deep_model_id": deep_model_id,
        }


def run_widesearch_batch(
    input_file: str,
    main_model_id: str,
    tabular_model_id: str,
    deep_model_id: str,
    output_dir: str,
    db_name: str,
    clear_db: bool,
    use_summary_tool: bool,
    max_workers: int = 4,
    start_idx: int = 0,
    end_idx: Optional[int] = None,
    skip_completed: bool = True,
    timeout_seconds: int = 3600,
    tool_response_retention_budget: Optional[int] = None,
    max_tool_threads: Optional[int] = None,
    use_out_key: bool = False,
    global_visit_limit: Optional[int] = None,
    global_search_limit: Optional[int] = None,
    main_max_steps: int = 40,
    subagent_max_steps: int = 40,
    main_enable_context_summarization: bool = False,
    main_context_token_threshold: int = 80000,
    tabular_enable_context_summarization: bool = False,
    tabular_context_token_threshold: int = 60000,
    deep_enable_context_summarization: bool = False,
    deep_context_token_threshold: int = 60000,
    tabular_agent_limit: Optional[int] = None,
    deep_agent_limit: Optional[int] = None
):
    """è¿è¡Œ widesearch æ‰¹é‡æ¨ç†
    
    ä½¿ç”¨ multiprocessing å®ç°çœŸæ­£çš„è¶…æ—¶æ§åˆ¶ï¼š
    - æ¯ä¸ªä»»åŠ¡åœ¨ç‹¬ç«‹çš„å­è¿›ç¨‹ä¸­è¿è¡Œ
    - è¶…æ—¶åå¯ä»¥å¼ºåˆ¶ç»ˆæ­¢è¿›ç¨‹
    - ä½¿ç”¨çº¿ç¨‹æ± æ¥è°ƒåº¦å¤šä¸ªè¿›ç¨‹ï¼Œå®ç°å¹¶è¡Œæ‰§è¡Œ
    """
    console.print(f"\n[bold cyan]{'='*80}[/bold cyan]")
    console.print(f"[bold cyan]ğŸš€ Starting widesearch batch inference (with multiprocessing timeout)[/bold cyan]")
    console.print(f"[bold cyan]Input file: {input_file}[/bold cyan]")
    console.print(f"[bold cyan]Timeout per task: {timeout_seconds}s[/bold cyan]")
    console.print(f"[bold cyan]{'='*80}[/bold cyan]\n")
    
    # åŠ è½½æ•°æ®é›†
    examples = load_widesearch_dataset(input_file)
    
    if not examples:
        console.print(f"[bold red]âŒ No examples loaded from {input_file}[/bold red]")
        return
    
    # åˆ‡ç‰‡å¤„ç†
    if end_idx is None:
        end_idx = len(examples)
    examples = examples[start_idx:end_idx]
    
    console.print(f"[bold]Loaded {len(examples)} examples[/bold]")
    
    # åˆ›å»ºå·¥ä½œç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡
    if skip_completed:
        original_count = len(examples)
        examples = [ex for ex in examples if not is_task_completed(ex["instance_id"], output_path)]
        skipped_count = original_count - len(examples)
        if skipped_count > 0:
            console.print(f"[yellow]â­ï¸  Skipped {skipped_count} already completed tasks[/yellow]")
        console.print(f"[bold]Remaining tasks: {len(examples)}[/bold]")
    
    if not examples:
        console.print(f"[bold green]âœ… All tasks are already completed![/bold green]")
        return
    
    # å¦‚æœ clear_db=Trueï¼Œåœ¨æ‰¹é‡æ‰§è¡Œå¼€å§‹å‰åªæ¸…ç©ºä¸€æ¬¡æ•°æ®åº“
    if clear_db:
        console.print(f"[bold yellow]ğŸ§¹ Clearing database '{db_name}' before batch execution...[/bold yellow]")
        clear_database_tables(db_name)
        console.print(f"[bold green]âœ“ Database cleared. All tasks will use clear_db=False[/bold green]\n")
    
    # å¤„ç†ä»»åŠ¡
    results = []
    task_clear_db = False  # æ‰¹é‡æ‰§è¡Œæ—¶ï¼Œæ¯ä¸ªä»»åŠ¡éƒ½ä¸å†æ¸…ç©ºæ•°æ®åº“ï¼ˆå·²åœ¨å¼€å§‹å‰æ¸…ç©ºï¼‰
    
    console.print(f"[bold cyan]ğŸ”§ Using {max_workers} parallel workers[/bold cyan]")
    console.print(f"[bold cyan]ğŸ’¡ Each task runs in a separate process with enforced timeout[/bold cyan]\n")
    
    # ä½¿ç”¨çº¿ç¨‹æ± æ¥å¹¶è¡Œè°ƒåº¦å¤šä¸ªä»»åŠ¡ï¼ˆæ¯ä¸ªä»»åŠ¡åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­æ‰§è¡Œï¼‰
    # è¿™é‡Œçº¿ç¨‹åªè´Ÿè´£å¯åŠ¨å’Œç›‘æ§è¿›ç¨‹ï¼ŒçœŸæ­£çš„ä»»åŠ¡æ‰§è¡Œåœ¨å­è¿›ç¨‹ä¸­
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        futures = {
            executor.submit(
                run_single_task_with_timeout,
                example,
                main_model_id,
                tabular_model_id,
                deep_model_id,
                output_path,
                db_name,
                task_clear_db,
                use_summary_tool,
                tool_response_retention_budget,
                max_tool_threads,
                use_out_key,
                global_visit_limit,
                global_search_limit,
                main_max_steps,
                subagent_max_steps,
                main_enable_context_summarization,
                main_context_token_threshold,
                tabular_enable_context_summarization,
                tabular_context_token_threshold,
                deep_enable_context_summarization,
                deep_context_token_threshold,
                tabular_agent_limit,
                deep_agent_limit,
                timeout_seconds  # ä¼ é€’è¶…æ—¶å‚æ•°
            ): example for example in examples
        }
        
        console.print(f"[bold green]âœ… Submitted {len(futures)} tasks[/bold green]\n")
        
        # ä½¿ç”¨ rich Progress æ›¿ä»£ tqdm
        from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=console
        ) as progress:
            task = progress.add_task("Processing widesearch tasks", total=len(examples))
            
            for future in as_completed(futures):
                example = futures[future]
                try:
                    # è¿™é‡Œä¸éœ€è¦å†è®¾ç½® timeoutï¼Œå› ä¸º run_single_task_with_timeout
                    # å†…éƒ¨ä½¿ç”¨ multiprocessing å·²ç»å®ç°äº†çœŸæ­£çš„è¶…æ—¶æ§åˆ¶
                    # ä½†æˆ‘ä»¬åŠ ä¸€ä¸ªé¢å¤–çš„ç¼“å†²æ—¶é—´ä»¥é˜²ä¸‡ä¸€
                    result = future.result(timeout=timeout_seconds + 120)
                    
                    # åå¤„ç†ï¼šå¦‚æœç»“æœæ— æ•ˆï¼Œå°è¯•ä»æ•°æ®åº“è¡¨æ ¼æ¢å¤
                    result = postprocess_result(
                        result=result,
                        db_name=db_name,
                        output_dir=output_path,
                        connection_string=os.getenv("MONGODB_CONNECTION_STRING", "mongodb://localhost:27017/")
                    )
                    
                    results.append(result)
                except Exception as e:
                    console.print(f"[bold red]âŒ Error processing {example['instance_id']}: {e}[/bold red]")
                    traceback.print_exc()
                    error_result = {
                        "instance_id": example["instance_id"],
                        "task_id": example.get("task_id", example["instance_id"]),
                        "query": example.get("query", ""),
                        "error": str(e),
                        "timeout": False,
                        "start_time": datetime.now().isoformat(),
                    }
                    
                    # å¯¹äºé”™è¯¯ç»“æœä¹Ÿå°è¯•åå¤„ç†æ¢å¤
                    error_result = postprocess_result(
                        result=error_result,
                        db_name=db_name,
                        output_dir=output_path,
                        connection_string=os.getenv("MONGODB_CONNECTION_STRING", "mongodb://localhost:27017/")
                    )
                    
                    results.append(error_result)
                finally:
                    progress.update(task, advance=1)
    
    # ç»Ÿè®¡ç»“æœ
    completed = sum(1 for r in results if (r.get("response") or r.get("answer")) and not r.get("error"))
    timeout_count = sum(1 for r in results if r.get("timeout"))
    error_count = sum(1 for r in results if r.get("error") and not r.get("timeout"))
    
    console.print(f"\n[bold green]âœ… Completed widesearch batch: {len(results)}/{len(examples)} tasks[/bold green]")
    console.print(f"[bold]  - Successfully completed: {completed}[/bold]")
    console.print(f"[bold]  - Timeout: {timeout_count}[/bold]")
    console.print(f"[bold]  - Errors: {error_count}[/bold]")
    console.print(f"[bold]Results directory: {output_path}[/bold]")


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡è¿è¡Œ widesearch æ¨ç†æ¡†æ¶")
    
    # å¤ç”¨ run_widesearch_inference.py çš„å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument(
        "--main-model-id", 
        type=str, 
        default="us.anthropic.claude-sonnet-4-20250514-v1:0", 
        help="Main Agent çš„æ¨¡å‹ ID (é»˜è®¤: us.anthropic.claude-sonnet-4-20250514-v1:0)"
    )
    parser.add_argument(
        "--tabular-model-id", 
        type=str, 
        default="us.anthropic.claude-sonnet-4-20250514-v1:0", 
        help="Tabular Search Agent çš„æ¨¡å‹ ID (é»˜è®¤: us.anthropic.claude-sonnet-4-20250514-v1:0)"
    )
    parser.add_argument(
        "--deep-model-id", 
        type=str, 
        default="us.anthropic.claude-sonnet-4-20250514-v1:0", 
        help="Deep Search Agent çš„æ¨¡å‹ ID (é»˜è®¤: us.anthropic.claude-sonnet-4-20250514-v1:0)"
    )
    parser.add_argument("--output-dir", "-o", type=str, default="./output", help="è¾“å‡ºç›®å½• (é»˜è®¤: ./output)")
    parser.add_argument("--db-name", "-d", type=str, default="inference_db", help="æ•°æ®åº“åç§° (é»˜è®¤: inference_db)")
    parser.add_argument("--clear-db", action="store_true", default=True, help="æ¸…ç©ºæ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨ (é»˜è®¤: True)")
    parser.add_argument("--no-clear-db", dest="clear_db", action="store_false", help="ä¸æ¸…ç©ºæ•°æ®åº“")
    parser.add_argument("--use-summary-tool", action="store_true", default=False, help="ä½¿ç”¨å¸¦æ‘˜è¦åŠŸèƒ½çš„ç½‘é¡µè®¿é—®å·¥å…· (JinaBackedVisitWebpageSummaryTool)")
    parser.add_argument("--use-out-key", action="store_true", default=False, help="ä½¿ç”¨ OUT_API_KEY å’Œ OUT_BASE_URL è€Œä¸æ˜¯ OPENAI_API_KEY å’Œ OPENAI_BASE_URL (é»˜è®¤: False)")
    
    # æ–°å¢æ‰¹é‡å¤„ç†å‚æ•°
    parser.add_argument("--input-file", "-i", type=str, 
                       default=str(BASE_DIR / "benchmark" / "widesearch" / "widesearch.jsonl"),
                       help="è¾“å…¥æ–‡ä»¶è·¯å¾„ (é»˜è®¤: benchmark/widesearch/widesearch.jsonl)")
    parser.add_argument("--max-workers", "-w", type=int, default=4,
                       help="æœ€å¤§å¹¶å‘æ•° (é»˜è®¤: 4)")
    parser.add_argument("--start-idx", type=int, default=0,
                       help="å¼€å§‹ç´¢å¼• (é»˜è®¤: 0)")
    parser.add_argument("--tool-response-retention-budget", type=int, default=None, help="å·¥å…·å“åº”ä¿ç•™é¢„ç®— (é»˜è®¤: None)")
    parser.add_argument(
        "--max-tool-threads", 
        type=int, 
        default=4, 
        help="æœ€å¤§å¹¶è¡Œå·¥å…·è°ƒç”¨çº¿ç¨‹æ•°ï¼Œç”¨äºæ§åˆ¶å¹¶è¡Œå·¥å…·è°ƒç”¨çš„å¹¶å‘åº¦ï¼Œé¿å… API rate-limit (é»˜è®¤: Noneï¼Œä½¿ç”¨ ThreadPoolExecutor é»˜è®¤å€¼)"
    )
    parser.add_argument("--end-idx", type=int, default=None,
                       help="ç»“æŸç´¢å¼• (é»˜è®¤: None, å¤„ç†æ‰€æœ‰)")
    parser.add_argument("--skip-completed", action="store_true", default=True,
                       help="è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡ (é»˜è®¤: True)")
    parser.add_argument("--no-skip-completed", dest="skip_completed", action="store_false",
                       help="ä¸è·³è¿‡å·²å®Œæˆçš„ä»»åŠ¡")
    parser.add_argument("--timeout-seconds", type=int, default=3600,
                       help="ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰(é»˜è®¤: 3600, å³1å°æ—¶)")
    parser.add_argument(
        "--global-visit-limit",
        type=int,
        default=100,
        help="å…¨å±€ç½‘é¡µè®¿é—®æ¬¡æ•°é™åˆ¶ï¼Œæ‰€æœ‰ agent å…±äº«æ­¤é™åˆ¶ (é»˜è®¤: Noneï¼Œä¸é™åˆ¶)"
    )
    parser.add_argument(
        "--global-search-limit",
        type=int,
        default=100,
        help="å…¨å±€æœç´¢æ¬¡æ•°é™åˆ¶ï¼Œæ‰€æœ‰ agent å…±äº«æ­¤é™åˆ¶ (é»˜è®¤: Noneï¼Œä¸é™åˆ¶)"
    )
    parser.add_argument(
        "--main-max-step",
        type=int,
        default=40,
        help="Main Agent çš„æœ€å¤§æ­¥æ•°é™åˆ¶ (é»˜è®¤: 40)"
    )
    parser.add_argument(
        "--subagent-max-step",
        type=int,
        default=40,
        help="Sub Agent (Tabular Search Agent å’Œ Deep Search Agent) çš„æœ€å¤§æ­¥æ•°é™åˆ¶ (é»˜è®¤: 40)"
    )
    
    # Main Agent Context Summarization å‚æ•°
    parser.add_argument(
        "--main-enable-context-summarization",
        action="store_true",
        default=False,
        help="ä¸º Main Agent å¯ç”¨ context summarization åŠŸèƒ½ (é»˜è®¤: False)"
    )
    parser.add_argument(
        "--main-context-token-threshold",
        type=int,
        default=80000,
        help="Main Agent çš„ context summarization token é˜ˆå€¼ (é»˜è®¤: 80000)"
    )
    # Tabular Search Agent Context Summarization å‚æ•°
    parser.add_argument(
        "--tabular-enable-context-summarization",
        action="store_true",
        default=False,
        help="ä¸º Tabular Search Agent å¯ç”¨ context summarization åŠŸèƒ½ (é»˜è®¤: False)"
    )
    parser.add_argument(
        "--tabular-context-token-threshold",
        type=int,
        default=60000,
        help="Tabular Search Agent çš„ context summarization token é˜ˆå€¼ (é»˜è®¤: 60000)"
    )
    # Deep Search Agent Context Summarization å‚æ•°
    parser.add_argument(
        "--deep-enable-context-summarization",
        action="store_true",
        default=False,
        help="ä¸º Deep Search Agent å¯ç”¨ context summarization åŠŸèƒ½ (é»˜è®¤: False)"
    )
    parser.add_argument(
        "--deep-context-token-threshold",
        type=int,
        default=60000,
        help="Deep Search Agent çš„ context summarization token é˜ˆå€¼ (é»˜è®¤: 60000)"
    )
    # ä¾¿æ·å‚æ•°ï¼šåŒæ—¶ä¸ºæ‰€æœ‰ agent å¯ç”¨ context summarization
    parser.add_argument(
        "--enable-all-context-summarization",
        action="store_true",
        default=False,
        help="ä¸ºæ‰€æœ‰ agent (Main/Tabular/Deep) åŒæ—¶å¯ç”¨ context summarization åŠŸèƒ½ (é»˜è®¤: False)"
    )
    # Managed agent è°ƒç”¨æ¬¡æ•°é™åˆ¶å‚æ•°
    parser.add_argument(
        "--tabular-agent-limit",
        type=int,
        default=50,
        help="Tabular Search Agent çš„è°ƒç”¨æ¬¡æ•°é™åˆ¶ (é»˜è®¤: Noneï¼Œä¸é™åˆ¶)"
    )
    parser.add_argument(
        "--deep-agent-limit",
        type=int,
        default=50,
        help="Deep Search Agent çš„è°ƒç”¨æ¬¡æ•°é™åˆ¶ (é»˜è®¤: Noneï¼Œä¸é™åˆ¶)"
    )
    
    args = parser.parse_args()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if not os.getenv("OPENAI_BASE_URL"):
        console.print("[yellow]âš ï¸  Warning: OPENAI_BASE_URL not set in environment[/yellow]")
    if not os.getenv("OPENAI_API_KEY"):
        console.print("[yellow]âš ï¸  Warning: OPENAI_API_KEY not set in environment[/yellow]")
    
    # å¤„ç† enable-all-context-summarization ä¾¿æ·å‚æ•°
    main_enable_ctx_sum = args.main_enable_context_summarization or args.enable_all_context_summarization
    tabular_enable_ctx_sum = args.tabular_enable_context_summarization or args.enable_all_context_summarization
    deep_enable_ctx_sum = args.deep_enable_context_summarization or args.enable_all_context_summarization
    
    # è¿è¡Œæ‰¹é‡æ¨ç†
    run_widesearch_batch(
        input_file=args.input_file,
        main_model_id=args.main_model_id,
        tabular_model_id=args.tabular_model_id,
        deep_model_id=args.deep_model_id,
        output_dir=args.output_dir,
        db_name=args.db_name,
        clear_db=args.clear_db,
        use_summary_tool=args.use_summary_tool,
        max_workers=args.max_workers,
        start_idx=args.start_idx,
        end_idx=args.end_idx,
        skip_completed=args.skip_completed,
        timeout_seconds=args.timeout_seconds,
        tool_response_retention_budget=args.tool_response_retention_budget,
        max_tool_threads=args.max_tool_threads,
        use_out_key=args.use_out_key,
        global_visit_limit=args.global_visit_limit,
        global_search_limit=args.global_search_limit,
        main_max_steps=args.main_max_step,
        subagent_max_steps=args.subagent_max_step,
        main_enable_context_summarization=main_enable_ctx_sum,
        main_context_token_threshold=args.main_context_token_threshold,
        tabular_enable_context_summarization=tabular_enable_ctx_sum,
        tabular_context_token_threshold=args.tabular_context_token_threshold,
        deep_enable_context_summarization=deep_enable_ctx_sum,
        deep_context_token_threshold=args.deep_context_token_threshold,
        tabular_agent_limit=args.tabular_agent_limit,
        deep_agent_limit=args.deep_agent_limit
    )


if __name__ == "__main__":
    # è®¾ç½® multiprocessing å¯åŠ¨æ–¹æ³•
    # åœ¨ macOS ä¸Šä½¿ç”¨ 'spawn' (Python 3.8+ é»˜è®¤)
    # åœ¨ Linux ä¸Šå¯ä»¥ä½¿ç”¨ 'fork' ä½† 'spawn' æ›´å®‰å…¨
    # è¿™ç¡®ä¿å­è¿›ç¨‹æœ‰å¹²å‡€çš„çŠ¶æ€ï¼Œé¿å…æ½œåœ¨çš„é—®é¢˜
    try:
        multiprocessing.set_start_method('spawn', force=False)
    except RuntimeError:
        # å¦‚æœå·²ç»è®¾ç½®è¿‡ï¼Œå¿½ç•¥
        pass
    
    main()

