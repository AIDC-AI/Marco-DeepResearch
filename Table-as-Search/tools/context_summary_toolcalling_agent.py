#!/usr/bin/env python3
# Copyright (C) 2026 AIDC-AI
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#     http://www.apache.org/licenses/LICENSE-2.0
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Context Summarization ToolCalling Agent

ä¸€ä¸ªå¸¦æœ‰è‡ªåŠ¨ä¸Šä¸‹æ–‡æ‘˜è¦åŠŸèƒ½çš„ ToolCallingAgentï¼Œå½“ä¸Šä¸‹æ–‡ token æ•°é‡è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œ
è‡ªåŠ¨æ‰§è¡Œ context summarizationï¼Œæ¸…ç©ºä¹‹å‰çš„ history å¹¶å°† summarization ä½œä¸ºæœ€æ–°çš„å†å²ã€‚

ä½¿ç”¨æ–¹æ³•:
    # åœ¨ run_widesearch_inference.py ä¸­ä½¿ç”¨:
    from tools.context_summary_toolcalling_agent import create_context_summarization_agent_class, SummaryStep
    
    # åˆ›å»ºå¸¦æœ‰ context summarization åŠŸèƒ½çš„ agent ç±»
    ContextSummarizationAgent = create_context_summarization_agent_class(MemoryManagedToolCallingAgent)
    
    # ä½¿ç”¨è¿™ä¸ªç±»åˆ›å»º agent
    main_agent = ContextSummarizationAgent(
        model=main_model,
        tools=main_agent_tools,
        context_token_threshold=80000,  # 80k token é˜ˆå€¼
        ...
    )
"""

import time
from typing import Any, Generator, Type, Optional
from dataclasses import dataclass

from loguru import logger as loguru_logger

from smolagents import ToolCallingAgent, Tool, Model
from smolagents.memory import MemoryStep, TaskStep, ActionStep, PlanningStep, AgentMemory
from smolagents.models import ChatMessage, MessageRole
from smolagents.monitoring import TokenUsage, Timing, LogLevel

# å¯¼å…¥ request_api_detail
try:
    from .my_utils import request_api_detail
except ImportError:
    try:
        from my_utils import request_api_detail
    except ImportError:
        request_api_detail = None


# =============================================================================
# SummaryStep: ç”¨äºå­˜å‚¨ context summarization ç»“æœçš„ MemoryStep
# =============================================================================
@dataclass
class SummaryStep(MemoryStep):
    """å­˜å‚¨ context summarization ç»“æœçš„æ­¥éª¤"""
    original_task: str
    summary: str
    summarized_steps_count: int
    tokens_before_summary: int = 0
    
    def to_messages(self, summary_mode: bool = False) -> list[ChatMessage]:
        """å°† SummaryStep è½¬æ¢ä¸ºæ¶ˆæ¯åˆ—è¡¨"""
        return [
            ChatMessage(
                role=MessageRole.USER,
                content=[{
                    "type": "text",
                    "text": f"""[ä¸Šä¸‹æ–‡æ‘˜è¦ - å·²å‹ç¼© {self.summarized_steps_count} ä¸ªå†å²æ­¥éª¤]

**åŸå§‹ä»»åŠ¡:** {self.original_task}

**ä¹‹å‰å·¥ä½œçš„æ‘˜è¦:**
{self.summary}

---
è¯·åŸºäºä»¥ä¸Šæ‘˜è¦ç»§ç»­å®Œæˆä»»åŠ¡ã€‚å¦‚æœæ‘˜è¦ä¸­çš„ä¿¡æ¯ä¸å®Œæ•´ï¼Œä½ å¯èƒ½éœ€è¦é‡æ–°æœç´¢æˆ–éªŒè¯ç›¸å…³ä¿¡æ¯ã€‚
"""
                }]
            )
        ]
    
    def dict(self):
        return {
            "original_task": self.original_task,
            "summary": self.summary,
            "summarized_steps_count": self.summarized_steps_count,
            "tokens_before_summary": self.tokens_before_summary,
        }


# =============================================================================
# Default Summarization Prompt (ä¸­æ–‡ç‰ˆ)
# =============================================================================
DEFAULT_SUMMARY_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI Agent ä¸Šä¸‹æ–‡æ‘˜è¦ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯å°† Agent çš„å†å²äº¤äº’è®°å½•å‹ç¼©æˆç»“æ„åŒ–çš„æ‘˜è¦ï¼Œä»¥ä¾¿ Agent èƒ½å¤Ÿåœ¨æœ‰é™çš„ä¸Šä¸‹æ–‡çª—å£ä¸­ç»§ç»­é«˜æ•ˆåœ°å®Œæˆä»»åŠ¡ã€‚

## æ‘˜è¦è¾“å‡ºæ ¼å¼è¦æ±‚

ä½ çš„æ‘˜è¦å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹å››ä¸ªéƒ¨åˆ†ç»„ç»‡ï¼Œæ¯ä¸ªéƒ¨åˆ†ç”¨æ¸…æ™°çš„æ ‡é¢˜åˆ†éš”ï¼š

### ä¸€ã€äº¤äº’å†å²ä¸è®¡åˆ’å›é¡¾
æ€»ç»“ Agent ä¹‹å‰çš„ä¸»è¦äº¤äº’è¿‡ç¨‹ï¼š
- æ‰§è¡Œäº†å“ªäº›æœç´¢å’Œç½‘é¡µè®¿é—®æ“ä½œ
- è°ƒç”¨äº†å“ªäº›å·¥å…·ï¼Œå¾—åˆ°äº†ä»€ä¹ˆç»“æœ
- ä¹‹å‰åˆ¶å®šçš„è®¡åˆ’ï¼ˆPlanï¼‰æ˜¯ä»€ä¹ˆ
- é‡åˆ°äº†å“ªäº›é—®é¢˜æˆ–é”™è¯¯
- **æœ‰å“ªäº›å¯ä»¥å¾…æ’å…¥æˆ–è€…æ›´æ–°çš„æ•°æ®ï¼Œä½†æ˜¯è¿˜æ²¡æœ‰æ’å…¥æ•°æ®åº“æˆ–è€…æ›´æ–°æ•°æ®åº“ï¼Œè¯·è®°å½•ä»–ä»¬çš„ä¿¡æ¯åƒä¸‡ä¸èƒ½é—æ¼ï¼Œè¦å°½å¯èƒ½å¤šçš„è®°å½•**

### äºŒã€å·²æ”¶é›†çš„è¡¨æ ¼æ•°æ®
è¯¦ç»†åˆ—å‡º Agent å·²ç»æ’å…¥æˆ–æ›´æ–°åˆ°æ•°æ®åº“è¡¨æ ¼ä¸­çš„æ•°æ®ï¼š
- ä½¿ç”¨ `add_records` æˆ– `update_records` å·¥å…·æ·»åŠ /æ›´æ–°äº†å“ªäº›è®°å½•
- æ¯æ¡è®°å½•çš„å…³é”®å­—æ®µå€¼æ˜¯ä»€ä¹ˆ
- å¦‚æœå¯èƒ½ï¼Œä»¥è¡¨æ ¼å½¢å¼å‘ˆç°å·²æ”¶é›†çš„æ•°æ®
- **åŠ¡å¿…ä¿ç•™æ‰€æœ‰å…·ä½“çš„æ•°å€¼ã€åç§°ã€æ—¥æœŸç­‰ç»“æ„åŒ–æ•°æ®ï¼Œè¿™äº›æ˜¯æœ€é‡è¦çš„ä¿¡æ¯**

### ä¸‰ã€æ›´æ–°åçš„è®¡åˆ’
åŸºäºå½“å‰è¿›å±•ï¼Œç»™å‡ºæ¥ä¸‹æ¥åº”è¯¥æ‰§è¡Œçš„è®¡åˆ’ï¼š
- ä»»åŠ¡å®Œæˆäº†å¤šå°‘ï¼Œè¿˜å‰©å¤šå°‘
- æ¥ä¸‹æ¥éœ€è¦æœç´¢æˆ–éªŒè¯ä»€ä¹ˆä¿¡æ¯
- éœ€è¦å¡«å……è¡¨æ ¼çš„å“ªäº›å­—æ®µ
- å»ºè®®çš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### å››ã€å…¶ä»–é‡è¦ä¿¡æ¯
è®°å½•å…¶ä»–å¯¹ç»§ç»­ä»»åŠ¡æœ‰å¸®åŠ©çš„ä¿¡æ¯ï¼š
- é‡è¦çš„å‘ç°æˆ–ç»“è®º
- éœ€è¦æ³¨æ„çš„çº¦æŸæ¡ä»¶
- å¤±è´¥çš„å°è¯•ï¼ˆé¿å…é‡å¤ï¼‰
- ä»»ä½•å…¶ä»–ä¸Šä¸‹æ–‡ä¿¡æ¯

## é‡è¦åŸåˆ™

1. **æ•°æ®ä¼˜å…ˆ**ï¼šæ‰€æœ‰å…·ä½“çš„æ•°æ®å€¼ï¼ˆæ•°å­—ã€åç§°ã€æ—¥æœŸã€é‡‘é¢ç­‰ï¼‰å¿…é¡»å®Œæ•´ä¿ç•™ï¼Œä¸èƒ½é—æ¼æˆ–æ¦‚æ‹¬
2. **ç»“æ„æ¸…æ™°**ï¼šä¸¥æ ¼æŒ‰ç…§å››ä¸ªéƒ¨åˆ†ç»„ç»‡è¾“å‡º
3. **ç®€æ´é«˜æ•ˆ**ï¼šåœ¨ä¿ç•™å…³é”®ä¿¡æ¯çš„å‰æä¸‹ï¼Œå°½é‡ç²¾ç®€æè¿°
4. **å¯æ“ä½œæ€§**ï¼šæ‘˜è¦åº”è¯¥è®© Agent èƒ½å¤Ÿç«‹å³ç»§ç»­å·¥ä½œï¼Œä¸éœ€è¦é‡æ–°æœç´¢å·²æœ‰çš„ä¿¡æ¯
"""

DEFAULT_SUMMARY_USER_TEMPLATE = """è¯·å¯¹ä»¥ä¸‹ Agent çš„å†å²äº¤äº’è®°å½•è¿›è¡Œæ‘˜è¦ã€‚Agent æ­£åœ¨æ‰§è¡Œä¸€ä¸ªæ•°æ®æ”¶é›†ä»»åŠ¡ï¼Œéœ€è¦ä½ ç”Ÿæˆç»“æ„åŒ–çš„æ‘˜è¦ä»¥ä¾¿ç»§ç»­å·¥ä½œã€‚

=== åŸå§‹ä»»åŠ¡ ===
{task}

=== å†å²äº¤äº’è®°å½• ===
{history}

=== å†å²è®°å½•ç»“æŸ ===

è¯·ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿæç¤ºä¸­è§„å®šçš„å››ä¸ªéƒ¨åˆ†ï¼ˆäº¤äº’å†å²ä¸è®¡åˆ’å›é¡¾ã€å·²æ”¶é›†çš„è¡¨æ ¼æ•°æ®ã€æ›´æ–°åçš„è®¡åˆ’ã€å…¶ä»–é‡è¦ä¿¡æ¯ï¼‰ç”Ÿæˆæ‘˜è¦ã€‚

ç‰¹åˆ«æ³¨æ„ï¼š
1. æ‰€æœ‰å·²ç»æ·»åŠ åˆ°è¡¨æ ¼ä¸­çš„æ•°æ®å¿…é¡»å®Œæ•´åˆ—å‡ºï¼Œä¸èƒ½é—æ¼
2. æ‰€æœ‰å…·ä½“çš„æ•°å€¼ã€åç§°ã€æ—¥æœŸç­‰å¿…é¡»åŸæ ·ä¿ç•™
3. å¦‚æœæœ‰è¡¨æ ¼æ•°æ®ï¼Œè¯·ä»¥ Markdown è¡¨æ ¼æ ¼å¼å‘ˆç°
"""


# =============================================================================
# Mixin ç±»: æ·»åŠ  Context Summarization åŠŸèƒ½
# =============================================================================
class ContextSummarizationMixin:
    """
    Context Summarization Mixinï¼Œå¯ä»¥æ·»åŠ åˆ°ä»»ä½• ToolCallingAgent å­ç±»ã€‚
    
    è¿™ä¸ª Mixin æä¾›è‡ªåŠ¨ context summarization åŠŸèƒ½ï¼š
    - è·Ÿè¸ªæ¯æ¬¡æ¨¡å‹è°ƒç”¨çš„ input token æ•°é‡
    - å½“ token æ•°é‡è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œè‡ªåŠ¨ç”Ÿæˆæ‘˜è¦å¹¶æ›¿æ¢å†å²
    - ä½¿ç”¨ qwen3-next-80b æ¨¡å‹è¿›è¡Œæ‘˜è¦ç”Ÿæˆ
    
    ä½¿ç”¨æ–¹æ³•:
        class MyAgent(ContextSummarizationMixin, ToolCallingAgent):
            pass
    """
    
    # éœ€è¦å­ç±»è®¾ç½®çš„å±æ€§
    context_token_threshold: int = 80000
    summary_model_name: str = "qwen3-next-80b-a3b-instruct"
    summary_system_prompt: str = DEFAULT_SUMMARY_SYSTEM_PROMPT
    summary_user_template: str = DEFAULT_SUMMARY_USER_TEMPLATE
    min_steps_before_summary: int = 5
    summary_timeout: float = 180.0
    summary_temperature: float = 0.0
    summary_max_retries: int = 5
    
    # ç»Ÿè®¡ä¿¡æ¯
    _summarization_count: int = 0
    _total_tokens_saved: int = 0
    
    def _init_summarization(
        self,
        context_token_threshold: int = 80000,
        summary_model_name: str = "qwen3-next-80b-a3b-instruct",
        summary_system_prompt: str = None,
        summary_user_template: str = None,
        min_steps_before_summary: int = 5,
        summary_timeout: float = 180.0,
        summary_temperature: float = 0.0,
        summary_max_retries: int = 5,
        # ä¿ç•™æ—§å‚æ•°å…¼å®¹æ€§ï¼ˆå·²å¼ƒç”¨ï¼‰
        summarization_model: Model = None,
    ):
        """åˆå§‹åŒ– summarization ç›¸å…³é…ç½®"""
        self.context_token_threshold = context_token_threshold
        self.summary_model_name = summary_model_name
        self.summary_system_prompt = summary_system_prompt or DEFAULT_SUMMARY_SYSTEM_PROMPT
        self.summary_user_template = summary_user_template or DEFAULT_SUMMARY_USER_TEMPLATE
        self.min_steps_before_summary = min_steps_before_summary
        self.summary_timeout = summary_timeout
        self.summary_temperature = summary_temperature
        self.summary_max_retries = summary_max_retries
        
        self._summarization_count = 0
        self._total_tokens_saved = 0
        
        # æ£€æŸ¥ request_api_detail æ˜¯å¦å¯ç”¨
        if request_api_detail is None:
            self.logger.log(
                "[ContextSummarization] âš ï¸ ä¸¥é‡è­¦å‘Š: request_api_detail ä¸å¯ç”¨! "
                "Context summarization åŠŸèƒ½å°†æ— æ³•æ­£å¸¸å·¥ä½œ!",
                level=LogLevel.ERROR
            )
            loguru_logger.error("[ContextSummarization] âš ï¸ request_api_detail ä¸å¯ç”¨! Context summarization åŠŸèƒ½å°†æ— æ³•æ­£å¸¸å·¥ä½œ!")
        
        self.logger.log(
            f"[ContextSummarization] Initialized with token threshold: {context_token_threshold:,}, "
            f"summary model: {summary_model_name}, max_retries: {summary_max_retries}",
            level=LogLevel.INFO
        )
        loguru_logger.info(f"[ContextSummarization] åˆå§‹åŒ–å®Œæˆ | tokené˜ˆå€¼: {context_token_threshold:,} | æ¨¡å‹: {summary_model_name} | æœ€å¤§é‡è¯•: {summary_max_retries}")
    
    def _get_last_input_tokens(self) -> int:
        """è·å–æœ€è¿‘ä¸€æ¬¡æ¨¡å‹è°ƒç”¨çš„ input token æ•°é‡"""
        for step in reversed(self.memory.steps):
            if isinstance(step, (ActionStep, PlanningStep)):
                if step.token_usage is not None:
                    return step.token_usage.input_tokens
        return 0
    
    def _count_action_steps(self) -> int:
        """ç»Ÿè®¡å½“å‰ memory ä¸­çš„ ActionStep æ•°é‡"""
        return sum(1 for step in self.memory.steps if isinstance(step, ActionStep))
    
    def _should_summarize(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œ context summarization"""
        last_input_tokens = self._get_last_input_tokens()
        action_step_count = self._count_action_steps()
        
        should_summarize = (
            last_input_tokens > self.context_token_threshold and 
            action_step_count >= self.min_steps_before_summary
        )
        
        loguru_logger.debug(f"[ContextSummarization] æ£€æŸ¥æ˜¯å¦éœ€è¦æ‘˜è¦ | input_tokens: {last_input_tokens:,} | threshold: {self.context_token_threshold:,} | steps: {action_step_count} | éœ€è¦æ‘˜è¦: {should_summarize}")
        
        if should_summarize:
            self.logger.log(
                f"[ContextSummarization] Triggering: "
                f"input_tokens={last_input_tokens:,} > threshold={self.context_token_threshold:,}, "
                f"steps={action_step_count}",
                level=LogLevel.INFO
            )
            loguru_logger.warning(f"[ContextSummarization] ğŸš¨ è§¦å‘ä¸Šä¸‹æ–‡æ‘˜è¦! | input_tokens: {last_input_tokens:,} > threshold: {self.context_token_threshold:,} | steps: {action_step_count}")
        
        return should_summarize
    
    def _history_to_text(self) -> str:
        """å°†å½“å‰ memory ä¸­çš„å†å²è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"""
        history_parts = []
        
        for i, step in enumerate(self.memory.steps):
            if isinstance(step, TaskStep):
                history_parts.append(f"[ä»»åŠ¡] {step.task}")
            
            elif isinstance(step, SummaryStep):
                history_parts.append(f"[ä¹‹å‰çš„æ‘˜è¦] {step.summary}")
            
            elif isinstance(step, PlanningStep):
                history_parts.append(f"[è®¡åˆ’] {step.plan}")
            
            elif isinstance(step, ActionStep):
                parts = [f"[æ­¥éª¤ {step.step_number}]"]
                
                if step.model_output:
                    output = str(step.model_output)
                    if len(output) > 2000:
                        output = output[:2000] + "... (å·²æˆªæ–­)"
                    parts.append(f"Agent æ€è€ƒ: {output}")
                
                if step.tool_calls:
                    for tc in step.tool_calls:
                        args_str = str(tc.arguments)
                        if len(args_str) > 500:
                            args_str = args_str[:500] + "... (å·²æˆªæ–­)"
                        parts.append(f"å·¥å…·è°ƒç”¨: {tc.name}({args_str})")
                
                if step.observations:
                    obs = step.observations
                    if len(obs) > 3000:
                        obs = obs[:3000] + "... (å·²æˆªæ–­)"
                    parts.append(f"è§‚å¯Ÿç»“æœ: {obs}")
                
                if step.error:
                    parts.append(f"é”™è¯¯: {step.error}")
                
                history_parts.append("\n".join(parts))
        
        return "\n\n---\n\n".join(history_parts)
    
    def _call_summary_api_once(self, prompt: str) -> tuple[Optional[str], Optional[str], Optional[dict]]:
        """
        å•æ¬¡è°ƒç”¨ qwen3-next-80b æ¨¡å‹ç”Ÿæˆæ‘˜è¦
        
        Args:
            prompt: å®Œæ•´çš„æ‘˜è¦æç¤ºè¯
            
        Returns:
            (æ‘˜è¦æ–‡æœ¬, é”™è¯¯ä¿¡æ¯, usageä¿¡æ¯) - æˆåŠŸæ—¶è¿”å› (æ‘˜è¦, None, usage)ï¼Œå¤±è´¥æ—¶è¿”å› (None, é”™è¯¯ä¿¡æ¯, None)
            usage åŒ…å«: prompt_tokens, completion_tokens, total_tokens
        """
        if request_api_detail is None:
            loguru_logger.error("[ContextSummarization] request_api_detail ä¸å¯ç”¨ï¼Œæ— æ³•è°ƒç”¨æ‘˜è¦ API")
            return None, "request_api_detail ä¸å¯ç”¨", None
        
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {"role": "system", "content": self.summary_system_prompt},
            {"role": "user", "content": prompt}
        ]
        
        loguru_logger.info(f"[ContextSummarization] å¼€å§‹è°ƒç”¨æ‘˜è¦ API | æ¨¡å‹: {self.summary_model_name} | prompté•¿åº¦: {len(prompt)}")
        start_time = time.time()
        
        try:
            # ç›´æ¥è°ƒç”¨ APIï¼ˆrequest_api_detail å†…éƒ¨å·²æœ‰ retry æœºåˆ¶ï¼‰
            response, status_code, _ = request_api_detail(
                message=messages,
                temperature=self.summary_temperature,
                sample_num=1,
                index=0,
                model_name=self.summary_model_name,
                retry_num=1,  # å•æ¬¡è°ƒç”¨ä¸é‡è¯•ï¼Œç”±å¤–å±‚ _call_summary_api_with_retry æ§åˆ¶é‡è¯•
                retry_duration=1,
            )
            
            elapsed_time = time.time() - start_time
            
            if status_code != 200 or response is None:
                error_msg = f"API è¿”å›é”™è¯¯ (status_code={status_code})"
                loguru_logger.error(f"[ContextSummarization] API è°ƒç”¨å¤±è´¥ | status_code: {status_code} | è€—æ—¶: {elapsed_time:.2f}s")
                return None, error_msg, None
            
            # ä»å“åº”ä¸­æå–å†…å®¹å’Œ usage
            summary_content, usage = self._extract_api_response(response)
            
            if not summary_content:
                loguru_logger.error("[ContextSummarization] æ— æ³•ä» API å“åº”ä¸­æå–å†…å®¹")
                return None, "æ— æ³•ä»å“åº”ä¸­æå–å†…å®¹", None
            
            # æ„å»ºæ—¥å¿—ä¿¡æ¯ï¼ŒåŒ…å«ç²¾ç¡®çš„ token æ•°é‡
            completion_tokens = usage.get('completion_tokens', 'N/A') if usage else 'N/A'
            prompt_tokens = usage.get('prompt_tokens', 'N/A') if usage else 'N/A'
            total_tokens = usage.get('total_tokens', 'N/A') if usage else 'N/A'
            
            self.logger.log(
                f"[ContextSummarization] API è°ƒç”¨æˆåŠŸ. "
                f"Model: {self.summary_model_name}, "
                f"Elapsed: {elapsed_time:.2f}s, "
                f"Summary: {len(summary_content)} chars / {completion_tokens} tokens",
                level=LogLevel.INFO
            )
            loguru_logger.success(
                f"[ContextSummarization] âœ… API è°ƒç”¨æˆåŠŸ | æ¨¡å‹: {self.summary_model_name} | è€—æ—¶: {elapsed_time:.2f}s | "
                f"æ‘˜è¦: {len(summary_content)} chars / {completion_tokens} tokens | "
                f"prompt_tokens: {prompt_tokens} | total_tokens: {total_tokens}"
            )
            
            return summary_content, None, usage
            
        except Exception as e:
            elapsed_time = time.time() - start_time
            error_msg = f"API è°ƒç”¨å¼‚å¸¸: {str(e)}"
            loguru_logger.exception(f"[ContextSummarization] API è°ƒç”¨å¼‚å¸¸ | è€—æ—¶: {elapsed_time:.2f}s | é”™è¯¯: {e}")
            return None, error_msg, None
    
    def _call_summary_api_with_retry(self, prompt: str) -> tuple[Optional[str], Optional[dict]]:
        """
        ä½¿ç”¨ retry æœºåˆ¶è°ƒç”¨ qwen3-next-80b æ¨¡å‹ç”Ÿæˆæ‘˜è¦
        
        Args:
            prompt: å®Œæ•´çš„æ‘˜è¦æç¤ºè¯
            
        Returns:
            (æ‘˜è¦æ–‡æœ¬, usageä¿¡æ¯) - å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥åˆ™è¿”å› (None, None)
            usage åŒ…å«: prompt_tokens, completion_tokens, total_tokens
        """
        last_error = None
        loguru_logger.info(f"[ContextSummarization] å¼€å§‹ç”Ÿæˆæ‘˜è¦ (æœ€å¤šé‡è¯• {self.summary_max_retries} æ¬¡)")
        
        for attempt in range(1, self.summary_max_retries + 1):
            self.logger.log(
                f"[ContextSummarization] å°è¯•ç”Ÿæˆæ‘˜è¦ (ç¬¬ {attempt}/{self.summary_max_retries} æ¬¡)...",
                level=LogLevel.INFO
            )
            loguru_logger.info(f"[ContextSummarization] ç¬¬ {attempt}/{self.summary_max_retries} æ¬¡å°è¯•...")
            
            summary_text, error_msg, usage = self._call_summary_api_once(prompt)
            
            if summary_text:
                if attempt > 1:
                    self.logger.log(
                        f"[ContextSummarization] ç¬¬ {attempt} æ¬¡å°è¯•æˆåŠŸ!",
                        level=LogLevel.INFO
                    )
                    loguru_logger.success(f"[ContextSummarization] ç¬¬ {attempt} æ¬¡å°è¯•æˆåŠŸ!")
                return summary_text, usage
            
            last_error = error_msg
            self.logger.log(
                f"[ContextSummarization] ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: {error_msg}",
                level=LogLevel.ERROR
            )
            loguru_logger.warning(f"[ContextSummarization] ç¬¬ {attempt} æ¬¡å°è¯•å¤±è´¥: {error_msg}")
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•
            if attempt < self.summary_max_retries:
                wait_time = min(5 * attempt, 30)  # é€’å¢ç­‰å¾…æ—¶é—´ï¼Œæœ€å¤š30ç§’
                self.logger.log(
                    f"[ContextSummarization] ç­‰å¾… {wait_time} ç§’åé‡è¯•...",
                    level=LogLevel.INFO
                )
                loguru_logger.info(f"[ContextSummarization] ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                time.sleep(wait_time)
        
        self.logger.log(
            f"[ContextSummarization] âŒ æ‰€æœ‰ {self.summary_max_retries} æ¬¡å°è¯•å‡å¤±è´¥! "
            f"æœ€åä¸€æ¬¡é”™è¯¯: {last_error}",
            level=LogLevel.ERROR
        )
        loguru_logger.error(f"[ContextSummarization] âŒ æ‰€æœ‰ {self.summary_max_retries} æ¬¡å°è¯•å‡å¤±è´¥! æœ€åé”™è¯¯: {last_error}")
        return None, None
    
    def _extract_api_response(self, response: dict) -> tuple[Optional[str], Optional[dict]]:
        """
        ä» API å“åº”ä¸­æå–å†…å®¹å’Œ usage ä¿¡æ¯
        
        Returns:
            (content, usage) - content æ˜¯æ‘˜è¦æ–‡æœ¬ï¼Œusage æ˜¯ token ä½¿ç”¨ä¿¡æ¯å­—å…¸
            usage åŒ…å«: prompt_tokens, completion_tokens, total_tokens
        """
        try:
            content = None
            usage = None
            
            # æå– content
            if "choices" in response and len(response["choices"]) > 0:
                choice = response["choices"][0]
                if "message" in choice and "content" in choice["message"]:
                    content = choice["message"]["content"].strip()
            
            # æå– usage
            if "usage" in response:
                usage = response["usage"]
                loguru_logger.debug(
                    f"[ContextSummarization] Token ä½¿ç”¨æƒ…å†µ: "
                    f"prompt_tokens={usage.get('prompt_tokens', 'N/A')}, "
                    f"completion_tokens={usage.get('completion_tokens', 'N/A')}, "
                    f"total_tokens={usage.get('total_tokens', 'N/A')}"
                )
            
            if not content:
                self.logger.log(
                    "[ContextSummarization] æ— æ³•ä»å“åº”ä¸­æå–å†…å®¹",
                    level=LogLevel.ERROR
                )
                return None, None
            
            return content, usage
            
        except Exception as e:
            self.logger.log(
                f"[ContextSummarization] æå–å“åº”å†…å®¹æ—¶å‡ºé”™: {str(e)}",
                level=LogLevel.ERROR
            )
            loguru_logger.exception(f"[ContextSummarization] æå–å“åº”å†…å®¹æ—¶å‡ºé”™: {e}")
            return None, None
    
    def _generate_context_summary(self) -> tuple[str, Optional[dict]]:
        """
        è°ƒç”¨ qwen3-next-80b æ¨¡å‹ç”Ÿæˆå½“å‰ context çš„æ‘˜è¦
        
        æ³¨æ„ï¼šåªä½¿ç”¨ qwen3-next-80b æ¨¡å‹ï¼Œä¸ä½¿ç”¨å…¶ä»–æ¨¡å‹ä½œä¸º fallbackã€‚
        å¦‚æœè°ƒç”¨å¤±è´¥ä¼šè‡ªåŠ¨é‡è¯•ï¼ˆæœ€å¤š summary_max_retries æ¬¡ï¼‰ã€‚
        
        Returns:
            (æ‘˜è¦æ–‡æœ¬, usageä¿¡æ¯) - usage åŒ…å« prompt_tokens, completion_tokens, total_tokens
        """
        # æ£€æŸ¥ request_api_detail æ˜¯å¦å¯ç”¨
        if request_api_detail is None:
            error_msg = (
                "[æ‘˜è¦ç”Ÿæˆå¤±è´¥: request_api_detail ä¸å¯ç”¨]\n\n"
                "è¯·ç¡®ä¿ my_utils æ¨¡å—æ­£ç¡®å®‰è£…å¹¶ä¸” request_api_detail å‡½æ•°å¯ç”¨ã€‚\n\n"
                f"åŸå§‹ä»»åŠ¡: {self.task}"
            )
            self.logger.log(
                "[ContextSummarization] âŒ æ— æ³•ç”Ÿæˆæ‘˜è¦: request_api_detail ä¸å¯ç”¨!",
                level=LogLevel.ERROR
            )
            loguru_logger.error("[ContextSummarization] âŒ æ— æ³•ç”Ÿæˆæ‘˜è¦: request_api_detail ä¸å¯ç”¨!")
            return error_msg, None
        
        history_text = self._history_to_text()
        loguru_logger.debug(f"[ContextSummarization] å†å²æ–‡æœ¬é•¿åº¦: {len(history_text)} å­—ç¬¦")
        
        # æ„å»ºç”¨æˆ·æç¤ºè¯
        user_prompt = self.summary_user_template.format(
            task=self.task or "æœªçŸ¥ä»»åŠ¡",
            history=history_text
        )
        
        self.logger.log(
            f"[ContextSummarization] æ­£åœ¨ä½¿ç”¨ {self.summary_model_name} ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦...",
            level=LogLevel.INFO
        )
        loguru_logger.info(f"[ContextSummarization] æ­£åœ¨ä½¿ç”¨ {self.summary_model_name} ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦... (prompt é•¿åº¦: {len(user_prompt)})")
        
        # ä½¿ç”¨å¸¦ retry æœºåˆ¶çš„ API è°ƒç”¨
        summary_text, usage = self._call_summary_api_with_retry(user_prompt)
        
        if summary_text:
            completion_tokens = usage.get('completion_tokens', 'N/A') if usage else 'N/A'
            self.logger.log(
                f"[ContextSummarization] æ‘˜è¦ç”Ÿæˆå®Œæˆ, é•¿åº¦: {len(summary_text)} å­—ç¬¦ / {completion_tokens} tokens",
                level=LogLevel.INFO
            )
            loguru_logger.success(f"[ContextSummarization] æ‘˜è¦ç”Ÿæˆå®Œæˆ | é•¿åº¦: {len(summary_text)} å­—ç¬¦ / {completion_tokens} tokens")
            return summary_text, usage
        else:
            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            error_msg = (
                f"[æ‘˜è¦ç”Ÿæˆå¤±è´¥: ç»è¿‡ {self.summary_max_retries} æ¬¡é‡è¯•åä»ç„¶å¤±è´¥]\n\n"
                f"è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’Œ API æœåŠ¡çŠ¶æ€ã€‚\n\n"
                f"åŸå§‹ä»»åŠ¡: {self.task}"
            )
            self.logger.log(
                f"[ContextSummarization] âŒ æ‘˜è¦ç”Ÿæˆæœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {self.summary_max_retries} æ¬¡",
                level=LogLevel.ERROR
            )
            loguru_logger.error(f"[ContextSummarization] âŒ æ‘˜è¦ç”Ÿæˆæœ€ç»ˆå¤±è´¥ï¼Œå·²é‡è¯• {self.summary_max_retries} æ¬¡")
            return error_msg, None
    
    def _perform_context_summarization(self):
        """æ‰§è¡Œ context summarization"""
        # è®°å½•å½“å‰çŠ¶æ€
        tokens_before = self._get_last_input_tokens()
        steps_count = len(self.memory.steps)
        original_task = self.task
        
        self.logger.log(
            f"[ContextSummarization] å¼€å§‹å‹ç¼©: "
            f"input_tokens={tokens_before:,}, æ­¥éª¤æ•°={steps_count}",
            level=LogLevel.INFO
        )
        loguru_logger.info(f"[ContextSummarization] ğŸ”„ å¼€å§‹ä¸Šä¸‹æ–‡å‹ç¼© | input_tokens: {tokens_before:,} | æ­¥éª¤æ•°: {steps_count}")
        
        # ç”Ÿæˆæ‘˜è¦
        summary, usage = self._generate_context_summary()
        
        # æå–ç²¾ç¡®çš„ token ä¿¡æ¯
        summary_completion_tokens = usage.get('completion_tokens', 0) if usage else 0
        summary_prompt_tokens = usage.get('prompt_tokens', 0) if usage else 0
        summary_total_tokens = usage.get('total_tokens', 0) if usage else 0
        
        # æ¸…ç©º memory
        self.memory.reset()
        loguru_logger.debug("[ContextSummarization] Memory å·²é‡ç½®")
        
        # æ·»åŠ  SummaryStep
        summary_step = SummaryStep(
            original_task=original_task,
            summary=summary,
            summarized_steps_count=steps_count,
            tokens_before_summary=tokens_before
        )
        self.memory.steps.append(summary_step)
        
        # é‡æ–°æ·»åŠ  TaskStep
        self.memory.steps.append(TaskStep(task=self.task))
        
        # æ›´æ–°ç»Ÿè®¡
        self._summarization_count += 1
        self._total_tokens_saved += tokens_before
        
        self.logger.log(
            f"[ContextSummarization] âœ… å®Œæˆ! (#{self._summarization_count}) "
            f"å‹ç¼©äº† {steps_count} ä¸ªæ­¥éª¤, "
            f"å‹ç¼©å‰ input_tokens: {tokens_before:,}, "
            f"æ‘˜è¦: {len(summary):,} å­—ç¬¦ / {summary_completion_tokens:,} tokens",
            level=LogLevel.INFO
        )
        loguru_logger.success(
            f"[ContextSummarization] âœ… ä¸Šä¸‹æ–‡å‹ç¼©å®Œæˆ! (ç¬¬ {self._summarization_count} æ¬¡) | "
            f"å‹ç¼©äº† {steps_count} ä¸ªæ­¥éª¤ | å‹ç¼©å‰ tokens: {tokens_before:,} | "
            f"æ‘˜è¦: {len(summary):,} å­—ç¬¦ / {summary_completion_tokens:,} tokens | "
            f"æ‘˜è¦ API ä½¿ç”¨: prompt={summary_prompt_tokens:,}, completion={summary_completion_tokens:,}, total={summary_total_tokens:,}"
        )
    
    def get_summarization_stats(self) -> dict:
        """è·å– summarization ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "summarization_count": self._summarization_count,
            "total_tokens_saved": self._total_tokens_saved,
            "current_steps": len(self.memory.steps),
            "context_token_threshold": self.context_token_threshold,
            "summary_model_name": self.summary_model_name,
        }


# =============================================================================
# å·¥å‚å‡½æ•°ï¼šåˆ›å»ºå¸¦æœ‰ Context Summarization åŠŸèƒ½çš„ Agent ç±»
# =============================================================================
def create_context_summarization_agent_class(base_class: Type[ToolCallingAgent]) -> Type:
    """
    åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ context summarization åŠŸèƒ½çš„ Agent ç±»ã€‚
    
    Args:
        base_class: åŸºç±»ï¼Œé€šå¸¸æ˜¯ ToolCallingAgent æˆ–å…¶å­ç±»ï¼ˆå¦‚ MemoryManagedToolCallingAgentï¼‰
    
    Returns:
        ä¸€ä¸ªæ–°çš„ç±»ï¼Œç»§æ‰¿è‡ª base_class å¹¶æ·»åŠ äº† context summarization åŠŸèƒ½
    
    ä½¿ç”¨ç¤ºä¾‹:
        # åœ¨ run_widesearch_inference.py ä¸­
        from tools.context_summary_toolcalling_agent import create_context_summarization_agent_class
        
        ContextSummarizationAgent = create_context_summarization_agent_class(MemoryManagedToolCallingAgent)
        
        main_agent = ContextSummarizationAgent(
            model=main_model,
            tools=main_agent_tools,
            context_token_threshold=80000,
            ...
        )
    """
    
    class ContextSummarizationAgent(ContextSummarizationMixin, base_class):
        """
        å¸¦æœ‰è‡ªåŠ¨ context summarization çš„ Agentã€‚
        
        å½“ä¸Šä¸‹æ–‡çš„ token æ•°é‡è¶…è¿‡é˜ˆå€¼æ—¶ï¼Œè‡ªåŠ¨æ‰§è¡Œ context summarizationï¼Œ
        æ¸…ç©ºä¹‹å‰çš„ history å¹¶å°† summarization ä½œä¸ºæœ€æ–°çš„å†å²ã€‚
        
        Args:
            tools: å·¥å…·åˆ—è¡¨
            model: æ¨¡å‹å®ä¾‹
            context_token_threshold: token é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼è§¦å‘ summarizationï¼ˆé»˜è®¤ 80000ï¼‰
            summary_model_name: ç”¨äºç”Ÿæˆæ‘˜è¦çš„æ¨¡å‹åç§°ï¼ˆé»˜è®¤ qwen3-next-80b-a3b-instructï¼‰
            summary_system_prompt: æ‘˜è¦ç³»ç»Ÿæç¤ºè¯
            summary_user_template: æ‘˜è¦ç”¨æˆ·æç¤ºè¯æ¨¡æ¿
            min_steps_before_summary: æœ€å°‘æ‰§è¡Œå¤šå°‘æ­¥åæ‰å…è®¸ summarizationï¼ˆé»˜è®¤ 5ï¼‰
            summary_timeout: æ‘˜è¦ API è°ƒç”¨è¶…æ—¶æ—¶é—´ï¼ˆé»˜è®¤ 180 ç§’ï¼‰
            summary_temperature: æ‘˜è¦ç”Ÿæˆæ¸©åº¦ï¼ˆé»˜è®¤ 0.0ï¼‰
            summary_max_retries: æ‘˜è¦ç”Ÿæˆå¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤ 5ï¼‰
            **kwargs: ä¼ é€’ç»™çˆ¶ç±»çš„å…¶ä»–å‚æ•°
        """
        
        def __init__(
            self,
            tools: list[Tool],
            model: Model,
            context_token_threshold: int = 80000,
            summary_model_name: str = "qwen3-next-80b-a3b-instruct",
            summary_system_prompt: str = None,
            summary_user_template: str = None,
            min_steps_before_summary: int = 5,
            summary_timeout: float = 180.0,
            summary_temperature: float = 0.0,
            summary_max_retries: int = 5,
            # ä¿ç•™æ—§å‚æ•°å…¼å®¹æ€§ï¼ˆå·²å¼ƒç”¨ï¼‰
            summarization_model: Model = None,
            **kwargs
        ):
            # å…ˆè°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
            super().__init__(tools=tools, model=model, **kwargs)
            
            # åˆå§‹åŒ– summarization åŠŸèƒ½
            self._init_summarization(
                context_token_threshold=context_token_threshold,
                summary_model_name=summary_model_name,
                summary_system_prompt=summary_system_prompt,
                summary_user_template=summary_user_template,
                min_steps_before_summary=min_steps_before_summary,
                summary_timeout=summary_timeout,
                summary_temperature=summary_temperature,
                summary_max_retries=summary_max_retries,
                summarization_model=summarization_model,
            )
        
        def _step_stream(self, memory_step: ActionStep) -> Generator:
            """é‡å†™ _step_streamï¼Œåœ¨æ¯æ­¥å¼€å§‹å‰æ£€æŸ¥æ˜¯å¦éœ€è¦ context summarization"""
            # åœ¨æ¯æ­¥å¼€å§‹å‰æ£€æŸ¥æ˜¯å¦éœ€è¦ summarization
            if self._should_summarize():
                self._perform_context_summarization()
            
            # è°ƒç”¨çˆ¶ç±»çš„ _step_stream ç»§ç»­æ‰§è¡Œ
            yield from super()._step_stream(memory_step)
    
    # è®¾ç½®ç±»åä»¥ä¾¿è°ƒè¯•
    ContextSummarizationAgent.__name__ = f"ContextSummarization{base_class.__name__}"
    ContextSummarizationAgent.__qualname__ = f"ContextSummarization{base_class.__name__}"
    
    return ContextSummarizationAgent


# =============================================================================
# ä¾¿æ·ç±»ï¼šç›´æ¥ç»§æ‰¿ ToolCallingAgentï¼ˆå¦‚æœä¸éœ€è¦ MemoryManagedToolCallingAgentï¼‰
# =============================================================================
class ContextSummarizationToolCallingAgent(ContextSummarizationMixin, ToolCallingAgent):
    """
    ç›´æ¥ç»§æ‰¿ ToolCallingAgent çš„ Context Summarization Agentã€‚
    
    å¦‚æœä½ éœ€è¦ç»§æ‰¿å…¶ä»–ç±»ï¼ˆå¦‚ MemoryManagedToolCallingAgentï¼‰ï¼Œ
    è¯·ä½¿ç”¨ create_context_summarization_agent_class() å·¥å‚å‡½æ•°ã€‚
    """
    
    def __init__(
        self,
        tools: list[Tool],
        model: Model,
        context_token_threshold: int = 80000,
        summary_model_name: str = "qwen3-next-80b-a3b-instruct",
        summary_system_prompt: str = None,
        summary_user_template: str = None,
        min_steps_before_summary: int = 5,
        summary_timeout: float = 180.0,
        summary_temperature: float = 0.0,
        summary_max_retries: int = 5,
        summarization_model: Model = None,
        **kwargs
    ):
        super().__init__(tools=tools, model=model, **kwargs)
        self._init_summarization(
            context_token_threshold=context_token_threshold,
            summary_model_name=summary_model_name,
            summary_system_prompt=summary_system_prompt,
            summary_user_template=summary_user_template,
            min_steps_before_summary=min_steps_before_summary,
            summary_timeout=summary_timeout,
            summary_temperature=summary_temperature,
            summary_max_retries=summary_max_retries,
            summarization_model=summarization_model,
        )
    
    def _step_stream(self, memory_step: ActionStep) -> Generator:
        if self._should_summarize():
            self._perform_context_summarization()
        yield from super()._step_stream(memory_step)


# =============================================================================
# æµ‹è¯•å’Œç¤ºä¾‹
# =============================================================================
if __name__ == "__main__":
    print("=" * 60)
    print("Context Summarization ToolCalling Agent")
    print("=" * 60)
    print()
    print("ä½¿ç”¨æ–¹æ³•:")
    print("  1. å¯¼å…¥å·¥å‚å‡½æ•°:")
    print("     from tools.context_summary_toolcalling_agent import \\")
    print("         create_context_summarization_agent_class, SummaryStep")
    print()
    print("  2. åˆ›å»º agent ç±»:")
    print("     ContextSummarizationAgent = create_context_summarization_agent_class(")
    print("         MemoryManagedToolCallingAgent")
    print("     )")
    print()
    print("  3. åˆ›å»º agent å®ä¾‹:")
    print("     agent = ContextSummarizationAgent(")
    print("         model=model,")
    print("         tools=tools,")
    print("         context_token_threshold=80000,")
    print("         summary_model_name='qwen3-next-80b-a3b-instruct',")
    print("         ...") 
    print("     )")
    print()
    print("åŠŸèƒ½ç‰¹æ€§:")
    print(f"  - é»˜è®¤ token é˜ˆå€¼: 80,000")
    print(f"  - ä½¿ç”¨ qwen3-next-80b-a3b-instruct æ¨¡å‹ç”Ÿæˆæ‘˜è¦")
    print(f"  - è‡ªåŠ¨å‹ç¼©è¶…é•¿ä¸Šä¸‹æ–‡")
    print(f"  - ä¿ç•™ä»»åŠ¡å’Œå…³é”®æ•°æ®")
    print(f"  - è·Ÿè¸ªæ‘˜è¦ç»Ÿè®¡ä¿¡æ¯")
    print()
    print("æ‘˜è¦è¾“å‡ºæ ¼å¼:")
    print("  ä¸€ã€äº¤äº’å†å²ä¸è®¡åˆ’å›é¡¾")
    print("  äºŒã€å·²æ”¶é›†çš„è¡¨æ ¼æ•°æ®")
    print("  ä¸‰ã€æ›´æ–°åçš„è®¡åˆ’")
    print("  å››ã€å…¶ä»–é‡è¦ä¿¡æ¯")
